{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import GradientBoostedTrees\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "from pyspark.mllib.linalg import Vectors, DenseVector, SparseVector\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn import metrics\n",
    "from tempfile import NamedTemporaryFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savm = sqlContext.sql(\"select * from edso_ignite.contest_savm\").repartition(100).cache()\n",
    "cr = sqlContext.sql(\"select * from edso_ignite.contest_cr\").repartition(100).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO optimize for only 1 select\n",
    "def apply_function(df, fields, function):\n",
    "    column_objects = []\n",
    "    for column in df.columns:\n",
    "        if column in fields:\n",
    "            column_objects.append(function(column))\n",
    "        else:\n",
    "            column_objects.append(column)\n",
    "    return df.select(column_objects)\n",
    "\n",
    "max_date = pd.datetime.today()\n",
    "\n",
    "#inputs to apply_function\n",
    "def function_binarize(null_value):\n",
    "    def inner(column):\n",
    "        return F.when(F.col(column) == null_value, 0).otherwise(1).alias(column)\n",
    "    return inner\n",
    "\n",
    "def function_binarize_nulls():\n",
    "    return lambda column : F.when(F.col(column).isNull(), 0).otherwise(1).alias(column)\n",
    "\n",
    "def parse_datetime():\n",
    "    def inner(column):\n",
    "        def parse(d):\n",
    "            if isinstance(d, str):\n",
    "                return(max_date - datetime.strptime(d, '%Y-%m-%d %H:%M:%S.0')).days\n",
    "            return d\n",
    "        return F.udf(parse, types.IntegerType())(F.col(column)).alias(column)\n",
    "    return inner\n",
    "\n",
    "def function_replace_empty_strings(value):\n",
    "    return lambda column : F.when(F.length(F.col(column)) == 0, value).otherwise(F.col(column)).alias(column)\n",
    "\n",
    "def function_replace_nulls(value):\n",
    "    return lambda column : F.when(F.col(column).isNull(), value).otherwise(F.col(column)).alias(column)\n",
    "\n",
    "def function_replace_nans(value):\n",
    "    return lambda column : F.when(F.col(column) == np.nan, value).otherwise(F.col(column)).alias(column)\n",
    "\n",
    "#because multidrop in pyspark is ugly\n",
    "def drop_columns(df, columns):\n",
    "    return df.select([c for c in df.columns if c not in columns])\n",
    "\n",
    "class FrequencyThreshold:\n",
    "    def __init__(self, df, columns):\n",
    "        self.freq_items = df.freqItems(columns, 0.01)\n",
    "        \n",
    "    def freq_threshold(self, column):\n",
    "        freq_items = self.freq_items.flatMap(lambda x : x[column+\"_freqItems\"]).collect()\n",
    "        return F.when(F.col(column).isin(freq_items), F.col(column)).otherwise('OTHER').alias(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_fields = ['party_id', 'parent_party_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[party_id: double, parent_party_id: double, party_name: string, node_type: string, address1: string, address2: string, address3: string, address4: string, city: string, county: string, state: string, province: string, postal_code: string, postal_code_extn: string, country_code: string, street_name: string, street_number: string, street_direction: string, street_type: string, geo_valid_status: string, completenes_status: string, cleansed_status: string, start_date: string, end_date: string, program_id: double, request_id: double, created_by: double, last_updated_by: double, creation_date: string, last_update_date: string, certified_date: string, site_expl_id: double, conflict_batch_id: double, sa_member_id: double, parent_sa_member_id: double, party_level: double, link_party_id: double, link_party_type: string, split_pct: double, sales_acct_id: double, operation_type: string, account_type: string, account_sub_type: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "builder = apply_function(savm, ['start_date', 'last_update_date', 'creation_date', 'certified_date'], parse_datetime())\n",
    "builder.write.saveAsTable('ignite.savm_parsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(party_id=1054733.0, parent_party_id=42499.0, party_name=u'PIZZA HUT, INC.', node_type=u'BR', address1=u'1821 SAN MARCO RD', address2=u'', address3=u'', address4=u'', city=u'MARCO ISLAND', county=u'COLLIER', state=u'FL', province=None, postal_code=u'34145', postal_code_extn=u'6722', country_code=u'US', street_name=u'SAN MARCO', street_number=u'1821', street_direction=None, street_type=u'RD', geo_valid_status=u'GEO_VALID', completenes_status=u'COMPLETE', cleansed_status=u'CLEANSED', start_date=u'2002-11-12 05:01:15.0', end_date=u'4712-12-31 00:00:00.0', program_id=44449.0, request_id=39462081.0, created_by=1116.0, last_updated_by=305153.0, creation_date=u'2002-11-12 05:01:15.0', last_update_date=u'2010-01-13 22:48:41.0', certified_date=u'2010-01-13 22:48:41.0', site_expl_id=479700768.0, conflict_batch_id=87615930.0, sa_member_id=3755080.0, parent_sa_member_id=-1.0, party_level=2.0, link_party_id=42499.0, link_party_type=u'ORGANIZATION', split_pct=50.0, sales_acct_id=252629163.0, operation_type=u'INSERT', account_type=u'NAMED_ACCOUNT', account_sub_type=u'DR', concat_address=u'1821 SAN MARCO RD    MARCO ISLAND COLLIER FL US 34145', address_tokenized=[u'1821', u'san', u'marco', u'rd', u'', u'', u'', u'marco', u'island', u'collier', u'fl', u'us', u'34145'], address_vector=SparseVector(8192, {0: 3.0, 3270: 1.0, 3634: 1.0, 3742: 1.0, 4202: 1.0, 4209: 1.0, 4682: 2.0, 6677: 1.0, 7136: 1.0, 7846: 1.0}), party_tokenized=[u'pizza', u'hut,', u'inc.'], party_vector=SparseVector(8192, {1040: 1.0, 3077: 1.0, 7304: 1.0})),\n",
       " Row(party_id=1063611.0, parent_party_id=222624.0, party_name=u'TCI MATERIALS MANAGEMENT, INC', node_type=u'BR', address1=u'2401 W VALLEY HWY N', address2=u'', address3=u'', address4=u'', city=u'AUBURN', county=u'KING', state=u'WA', province=None, postal_code=u'98001', postal_code_extn=u'1625', country_code=u'US', street_name=u'W VALLEY', street_number=u'2401', street_direction=u'NORTH', street_type=u'HWY', geo_valid_status=u'GEO_VALID', completenes_status=u'COMPLETE', cleansed_status=u'CLEANSED', start_date=u'2002-11-12 04:11:46.0', end_date=u'4712-12-31 00:00:00.0', program_id=44953.0, request_id=12206370.0, created_by=1116.0, last_updated_by=305158.0, creation_date=u'2002-11-12 04:11:46.0', last_update_date=u'2006-11-21 13:22:15.0', certified_date=u'2012-09-20 01:29:20.0', site_expl_id=487244133.0, conflict_batch_id=88208228.0, sa_member_id=3324524.0, parent_sa_member_id=-1.0, party_level=4.0, link_party_id=28471.0, link_party_type=u'ORGANIZATION', split_pct=100.0, sales_acct_id=265202409.0, operation_type=u'INSERT', account_type=u'NAMED_ACCOUNT', account_sub_type=u'DR', concat_address=u'2401 W VALLEY HWY N    AUBURN KING WA US 98001', address_tokenized=[u'2401', u'w', u'valley', u'hwy', u'n', u'', u'', u'', u'auburn', u'king', u'wa', u'us', u'98001'], address_vector=SparseVector(8192, {0: 3.0, 110: 1.0, 119: 1.0, 963: 1.0, 1027: 1.0, 2738: 1.0, 3742: 1.0, 3786: 1.0, 5450: 1.0, 7007: 1.0, 7063: 1.0}), party_tokenized=[u'tci', u'materials', u'management,', u'inc'], party_vector=SparseVector(8192, {44: 1.0, 4169: 1.0, 6110: 1.0, 8154: 1.0}))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_sparse(vectors):\n",
    "    values = defaultdict(float) # Dictionary with default value 0.0\n",
    "    # Add values from v1\n",
    "    for v in vectors:\n",
    "        for i in range(v.indices.size):\n",
    "            values[v.indices[i]] += v.values[i]\n",
    "    return Vectors.sparse(vectors[0].size, dict(values))\n",
    "\n",
    "def hstack_sparse(sparse_vectors):\n",
    "    values = {}\n",
    "    index = 0\n",
    "    for vector in sparse_vectors:\n",
    "        for i in range(vector.indices.shape[0]):\n",
    "            values[vector.indices[i] + index] = vector.values[i]\n",
    "        index += vector.size\n",
    "    return Vectors.sparse(index, values)\n",
    "\n",
    "def list_to_sparse(dense):\n",
    "    values = {}\n",
    "    for i, v in enumerate(dense):\n",
    "        values[i] = v\n",
    "    return SparseVector(len(dense), values)\n",
    "\n",
    "def pad_list(l, length, val = 0):\n",
    "    return l, length, val\n",
    "    return l + [val] * (length - len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def featurizer(result_iter):\n",
    "    rows = [row for row in result_iter]\n",
    "    \n",
    "    party_hq_counter = Counter()\n",
    "    \n",
    "    party_name_vectors = []\n",
    "    party_names = []\n",
    "    address_vectors = []\n",
    "    party_ids = []\n",
    "    for row in rows:\n",
    "        party_ids.append(row.party_id)\n",
    "        party_names.append(row.party_name)\n",
    "        party_hq_counter[row.parent_party_id] += 1\n",
    "        party_name_vectors.append(row.party_vector)\n",
    "        address_vectors.append(row.address_vector)\n",
    "    \n",
    "    return Row(sales_acct_id = rows[0].sales_acct_id,\n",
    "               party_ids = party_ids,\n",
    "               party_hq_counter = party_hq_counter,\n",
    "               party_name_vector = add_sparse(party_name_vectors),\n",
    "               address_vector = add_sparse(address_vectors),\n",
    "               num_parties = len(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered.where(F.col('sales_acct_id') == 203700707.0).map(lambda x : (x.sales_acct_id, x)).groupByKey().mapValues(featurizer) \\\n",
    "                    .saveAsPickleFile('savm_vectorized_4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
