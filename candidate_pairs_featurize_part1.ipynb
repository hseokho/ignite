{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import GradientBoostedTrees\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "from pyspark.mllib.linalg import Vectors, DenseVector, SparseVector\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "import operator\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"set spark.sql.shuffle.partitions=5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "party_values = [\n",
    "    'end_customer_party_ssot_party_id_int_sav_party_id',\n",
    "    'prior_party_ssot_party_id_int_sav_party_id',\n",
    "    'sol_branch_party',\n",
    "    'sol_gu_party',\n",
    "    'sol_hq_party',\n",
    "    'order_level_branch_party',\n",
    "    'order_level_gu_party',\n",
    "    'order_level_hq_party',\n",
    "    'line_level_branch_party',\n",
    "    'line_level_gu_party',\n",
    "    'line_level_hq_party',\n",
    "    'ship_to_branch_party',\n",
    "    'ship_to_gu_party',\n",
    "    'ship_to_hq_party',\n",
    "    'bill_to_branch_party',\n",
    "    'bill_to_gu_party',\n",
    "    'bill_to_hq_party',\n",
    "    'sold_to_branch_party',\n",
    "    'sold_to_gu_party',\n",
    "    'sold_to_hq_party'\n",
    "]\n",
    "\n",
    "address_fields = [\n",
    "    'address1', 'address2', 'address3', 'address4',\n",
    "    'city', 'county', 'state', 'postal_code', 'street_name',\n",
    "    'street_number', 'street_direction', 'street_type'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savm_parsed = sqlContext.sql(\"select * from ignite.savm_parsed\").repartition(100).cache()\n",
    "#temp because hadoop disk can't keep up...\n",
    "party_expansion = sqlContext.sql(\"select * from ignite.party_expansion_temp\").select(party_values +\n",
    "        [\n",
    "            'id', 'sales_acct_id', 'prior_party_name', 'end_customer_line_fix'\n",
    "        ]).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(end_customer_party_ssot_party_id_int_sav_party_id=217501313, prior_party_ssot_party_id_int_sav_party_id=217501313, sol_branch_party=217501313, sol_gu_party=2346701, sol_hq_party=2346701, order_level_branch_party=217501313, order_level_gu_party=2346701, order_level_hq_party=2346701, line_level_branch_party=217501313, line_level_gu_party=2346701, line_level_hq_party=2346701, ship_to_branch_party=217501313, ship_to_gu_party=2346701, ship_to_hq_party=2346701, bill_to_branch_party=128171, bill_to_gu_party=128171, bill_to_hq_party=128171, sold_to_branch_party=128171, sold_to_gu_party=128171, sold_to_hq_party=128171, id=68719483849, sales_acct_id=203728339.0, prior_party_name=u'j2 global communications', end_customer_line_fix=u'j2 global communications')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "party_expansion.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_candidate_gen = sqlContext.sql('select * from ignite.direct_id_candidates').unionAll(sqlContext.sql('select * from ignite.parent_expansion_candidates')).repartition(1000).drop_duplicates().cache()\n",
    "lsh_candidate_gen = sqlContext.sql('select * from ignite.lsh_savm_candidates_2').cache()\n",
    "candidate_match_status = sqlContext.sql('select * from ignite.candidate_match_status').cache()\n",
    "topic_modeling_savm = sqlContext.sql('select * from ignite.topic_modeling_savm_tfidf')\n",
    "topic_modeling_words = sqlContext.sql('select * from ignite.topic_modeling_per_word').cache()\n",
    "cr_parsed = sqlContext.sql(\"select * from ignite.cr_parsed\").repartition(500).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# thin out whatever we can\n",
    "def drop_columns(df, columns):\n",
    "    return df.select([c for c in df.columns if c not in columns])\n",
    "\n",
    "savm_parsed = drop_columns(savm_parsed, ['geo_valid_status', 'completenes_status', 'cleansed_status', 'start_date', 'end_date',\n",
    "                          'program_id', 'request_id', 'created_by', 'last_updated_by', 'creation_date', 'last_update_date',\n",
    "                          'certified_date', 'site_expl_id', 'conflict_batch_id', 'sa_member_id', 'parent_sa_member_id',\n",
    "                          'link_party_type', 'account_type', 'operation_type'\n",
    "                    ])\n",
    "cr_parsed = drop_columns(cr_parsed, ['geo_valid_status', 'completenes_status', 'cleansed_status', 'start_date', 'end_date',\n",
    "                          'program_id', 'request_id', 'created_by', 'last_updated_by', 'creation_date', 'last_update_date',\n",
    "                          'certified_date'\n",
    "                    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columnized_savm = sqlContext.sql('select * from ignite.temp_columnized_savm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collect_sales_acct_ids = columnized_savm.map(lambda x : x.savm_sales_acct_id).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_mapping = []\n",
    "for i, sales_acct_id in enumerate(collect_sales_acct_ids):\n",
    "    index_mapping.append((i, sales_acct_id))\n",
    "sqlContext.createDataFrame(pd.DataFrame(index_mapping, columns = ['broadcast_index', 'savm_sales_acct_id'])).write.saveAsTable('ignite.broadcast_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "broadcast_index_mapping = sqlContext.sql('select * from ignite.broadcast_index').map(lambda x : [x.broadcast_index, x.savm_sales_acct_id]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_mapping = {}\n",
    "for i, sales_acct_id in broadcast_index_mapping:\n",
    "    index_mapping[sales_acct_id] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savm_array_party_id = {}\n",
    "for row in columnized_savm.collect():\n",
    "    savm_array_party_id[index_mapping[row.savm_sales_acct_id]] = ' '.join([str(party_id) for party_id in row.savm_party_ids])\n",
    "    \n",
    "savm_broadcast_party_id = sc.broadcast(savm_array_party_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_savm_dicts = defaultdict(dict)\n",
    "keys = columnized_savm.take(1)[0].asDict().keys()\n",
    "for row in columnized_savm.collect():\n",
    "    for key in keys:\n",
    "        local_savm_dicts[key][row.savm_sales_acct_id] = row[key]\n",
    "\n",
    "savm_broadcasts = {}\n",
    "for key in keys:\n",
    "    savm_broadcasts[key] = sc.broadcast(local_savm_dicts[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_candidates = sqlContext.sql('select * from ignite.temp_joined_candidates_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_sparse(vectors):\n",
    "    values = defaultdict(float) # Dictionary with default value 0.0\n",
    "    # Add values from v1\n",
    "    for v in vectors:\n",
    "        for i in range(v.indices.size):\n",
    "            values[v.indices[i]] += v.values[i]\n",
    "    return Vectors.sparse(vectors[0].size, dict(values))\n",
    "\n",
    "def hstack_sparse(sparse_vectors):\n",
    "    values = {}\n",
    "    index = 0\n",
    "    for vector in sparse_vectors:\n",
    "        for i in range(vector.indices.shape[0]):\n",
    "            values[vector.indices[i] + index] = vector.values[i]\n",
    "        index += vector.size\n",
    "    return Vectors.sparse(index, values)\n",
    "\n",
    "def list_to_sparse(dense):\n",
    "    values = {}\n",
    "    for i, v in enumerate(dense):\n",
    "        values[i] = v\n",
    "    return SparseVector(len(dense), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def jaccard(set1, set2):\n",
    "    union_length = len(set1.union(set2))\n",
    "    if union_length == 0:\n",
    "        return 0\n",
    "    return float(len(set1.intersection(set2))) / union_length\n",
    "\n",
    "def set_tokenize(string):\n",
    "    if string == None:\n",
    "        return []\n",
    "    split = string.lower().replace(\".\", \"\").replace(\"-\", \" \").replace(\",\", \"\").split(\" \")\n",
    "    return set(split)\n",
    "\n",
    "def equality_check(str1, str2):\n",
    "    if str1 == None or str2 == None:\n",
    "        return 0.5\n",
    "    if str1 != str2:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "# credits to wikibooks\n",
    "def longest_common_substring(s1, s2):\n",
    "    m = [[0] * (1 + len(s2)) for i in xrange(1 + len(s1))]\n",
    "    longest, x_longest = 0, 0\n",
    "    for x in xrange(1, 1 + len(s1)):\n",
    "        for y in xrange(1, 1 + len(s2)):\n",
    "            if s1[x - 1] == s2[y - 1]:\n",
    "                m[x][y] = m[x - 1][y - 1] + 1\n",
    "                if m[x][y] > longest:\n",
    "                    longest = m[x][y]\n",
    "                    x_longest = x\n",
    "            else:\n",
    "                m[x][y] = 0\n",
    "    return s1[x_longest - longest: x_longest]\n",
    "\n",
    "def savm_topic_score(cleaned_name, savm_topic):\n",
    "    topic_scores = [Vectors.sparse(8, [0], [0])]\n",
    "    \n",
    "    unseen_words = 0\n",
    "    \n",
    "    for token in set_tokenize(cleaned_name):\n",
    "            \n",
    "        #idk how to fix the out of bounds error\n",
    "        if token in topic_modeling_words_broadcast.value:\n",
    "            word_data = topic_modeling_words_broadcast.value[token]\n",
    "\n",
    "            if word_data['index'] in savm_topic.tf.indices and savm_topic.tf[word_data['index']] > 0:\n",
    "\n",
    "                topic_vector = [\n",
    "                    word_data['count_docs'],\n",
    "                    savm_topic.normalized_tfidf[word_data['index']],\n",
    "                    savm_topic.tf[word_data['index']],\n",
    "                    word_data['gini'],\n",
    "                    word_data['min_tfidf'],\n",
    "                    word_data['avg_tfidf'],\n",
    "                    word_data['max_tfidf'],\n",
    "                ]\n",
    "                topic_scores.append(list_to_sparse(topic_vector))\n",
    "                \n",
    "        unseen_words += 1\n",
    "\n",
    "    if len(topic_scores) > 1:\n",
    "        topic_scores = topic_scores[1:] # remove the placeholder\n",
    "        \n",
    "    scored_vectors = [(topic[0], topic) for topic in topic_scores]\n",
    "    scored_vectors.sort(key = lambda x : x[0])\n",
    "    \n",
    "    final_subvector = hstack_sparse([concat_and_pad_vectors(scored_vectors), list_to_sparse([unseen_words])])\n",
    "    \n",
    "    return [min([point[0] for point in topic_scores]), final_subvector]\n",
    "\n",
    "def best_string_scores(full_row,  k = 5):\n",
    "    jaccard_scores = []\n",
    "    longest_run = 0\n",
    "    longest_string = None\n",
    "    \n",
    "    savm_cleaned_name = savm_broadcasts['savm_cleaned_name'].value[full_row.candidate_sales_acct_id_right]\n",
    "    \n",
    "    party_names = full_row.cr_cleaned_name + [full_row.prior_party_name, full_row.end_customer_line_fix]\n",
    "    \n",
    "    party_name_tokens = [set_tokenize(name) for name in party_names]\n",
    "    savm_party_name_tokens = [set_tokenize(name) for name in savm_cleaned_name]\n",
    "    \n",
    "    for i in range(len(party_names)):\n",
    "        for j in range(len(savm_cleaned_name)):\n",
    "            jaccard_scores.append(jaccard(party_name_tokens[i], savm_party_name_tokens[j]))\n",
    "            #lcs = longest_common_substring(party_names[i], savm_names[j]).strip()\n",
    "            #if len(lcs) > longest_run:\n",
    "            #    longest_run = len(lcs)\n",
    "            #    longest_string = lcs\n",
    "    \n",
    "    if len(jaccard_scores) < k:\n",
    "        jaccard_scores = jaccard_scores + [0] * (k - len(jaccard_scores))\n",
    "    \n",
    "    #return list_to_sparse([jaccard_score, longest_run, len(longest_string.split(\" \"))])\n",
    "    return list_to_sparse(jaccard_scores[:k])\n",
    "            \n",
    "def concat_and_pad_vectors(scored_vectors, k = 10):\n",
    "    empty_vector = Vectors.sparse(len(scored_vectors[0][1]), {})\n",
    "    \n",
    "    combined_pairwise_vectors = []\n",
    "    for i in range(k):\n",
    "        if i < len(scored_vectors):\n",
    "            combined_pairwise_vectors.append(scored_vectors[i][1])\n",
    "        else:\n",
    "            combined_pairwise_vectors.append(empty_vector)\n",
    "    \n",
    "    return hstack_sparse(combined_pairwise_vectors)\n",
    "\n",
    "def featurize_party_match_vector(full_row, k = 10):\n",
    "\n",
    "    #savm_parent_party_ids = savm_broadcasts['savm_parent_party_ids'].value[full_row.candidate_sales_acct_id_right]\n",
    "    savm_party_ids = savm_broadcast_party_ids.value[full_row.candidate_sales_acct_id_right]\n",
    "    return None\n",
    "    savm_parent_party_id_counter = Counter()\n",
    "    for parent_party_id in savm_parent_party_ids:\n",
    "        savm_parent_party_id_counter[parent_party_id] += 1\n",
    "    \n",
    "    party_match_vector = [0] * len(party_values)\n",
    "    hq_party_match_vector = [0] * len(party_values)\n",
    "    for i, party_value in enumerate(party_values):\n",
    "        party_match_vector[i] = 1 if full_row[party_value] in savm_parent_party_ids else 0\n",
    "        for j, key in enumerate(savm_parent_party_ids):\n",
    "            if key == full_row[party_value] and key != None:\n",
    "                hq_party_match_vector[i] = savm_parent_party_id_counter[key]\n",
    "        \n",
    "    party_match_vector = list_to_sparse(party_match_vector)\n",
    "    hq_party_match_vector = list_to_sparse(hq_party_match_vector)\n",
    "    \n",
    "        # count the number of candidates that are direct or hq matching\n",
    "    \n",
    "    num_party_match = 0\n",
    "    num_hq_party_match = 0\n",
    "    \n",
    "    savm_party_ids = savm_broadcasts['savm_party_ids'].value[full_row.candidate_sales_acct_id_right]\n",
    "\n",
    "    for cr_party in full_row.cr_party_ids:\n",
    "        for i, savm_party in enumerate(savm_party_ids):\n",
    "            if cr_party == savm_party:\n",
    "                num_party_match += 1\n",
    "            if cr_party == savm_parent_party_ids[i]:\n",
    "                num_hq_party_match += 1\n",
    "    \n",
    "    cr_savm_party_match_vector = list_to_sparse([num_party_match, num_hq_party_match])\n",
    "    \n",
    "    other_features = list_to_sparse([\n",
    "        len(full_row.cr_party_ids),\n",
    "        len(savm_party_ids)\n",
    "    ])\n",
    "\n",
    "    final_vector_stack = hstack_sparse([\n",
    "            other_features,\n",
    "            party_match_vector,\n",
    "            hq_party_match_vector,\n",
    "            cr_savm_party_match_vector\n",
    "        ])\n",
    "    \n",
    "    return (full_row.id, full_row.candidate_sales_acct_id_right, final_vector_stack)\n",
    "\n",
    "def featurize_topic_vector(full_row, k = 10):\n",
    "\n",
    "    # cr_i * savm_(all)\n",
    "    savm_topic = savm_topic_bc.value[full_row.candidate_sales_acct_id_right]\n",
    "    \n",
    "    scored_party_vectors = []\n",
    "    for cr_party_cleaned_name in full_row.cr_cleaned_name:\n",
    "        # score, topic vector\n",
    "        scored_party_vectors.append(savm_topic_score(cr_party_cleaned_name, savm_topic))\n",
    "\n",
    "    scored_party_vectors.sort(key = lambda x : x[0], reverse = True)\n",
    "    scored_party_vectors = concat_and_pad_vectors(scored_party_vectors, k = 5)\n",
    "    \n",
    "    fuzzy_vector = best_string_scores(full_row)    \n",
    "    \n",
    "    final_vector_stack = hstack_sparse([\n",
    "                                fuzzy_vector,\n",
    "                                scored_party_vectors, \n",
    "        ])\n",
    "    return (full_row.id, full_row.candidate_sales_acct_id_right, final_vector_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = joined_candidates.sample(False, 0.001).repartition(1000).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u\"Reference 'savm_sales_acct_id' is ambiguous, could be: savm_sales_acct_id#785, savm_sales_acct_id#1367.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-7cd6cee91576>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'select * from ignite.broadcast_index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'candidate_sales_acct_id_right'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'savm_sales_acct_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/mapr/spark/spark-1.6.1/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, other, on, how)\u001b[0m\n\u001b[0;32m    648\u001b[0m                 \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m                 \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inner\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m                 \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"how should be basestring\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/mapr/spark/spark-1.6.1/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/mapr/spark/spark-1.6.1/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     49\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: u\"Reference 'savm_sales_acct_id' is ambiguous, could be: savm_sales_acct_id#785, savm_sales_acct_id#1367.;\""
     ]
    }
   ],
   "source": [
    "sample = sample.join(sqlContext.sql('select * from ignite.broadcast_index'), on = F.col('candidate_sales_acct_id_right') == F.col('savm_sales_acct_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17044"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featured_pairs = sample.map(featurize_party_match_vector).cache()#.toDF(['id', 'candidate_sales_acct_id', 'feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17044"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featured_pairs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featured_pairs = joined_candidates.map(featurize_pairwise).toDF(['id', 'candidate_sales_acct_id', 'feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_truth_mapping = party_expansion.select(['id', F.col('sales_acct_id').alias('truth_sales_acct_id')]).drop_duplicates()\n",
    "builder = featured_pairs.join(id_truth_mapping, on = 'id', how = 'left')\n",
    "builder = builder.withColumn('label', F.when(F.col('candidate_sales_acct_id') == F.col('truth_sales_acct_id'), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1691"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "builder.write.saveAsTable('ignite.training_set_lsh', mode = 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set_id = sqlContext.sql('select * from ignite.training_set_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql('drop table ignite.training_set_lsh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
